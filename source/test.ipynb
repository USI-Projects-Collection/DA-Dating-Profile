{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fab00423",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split, ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "# 1) Paths & load\n",
    "DATA_DIR = Path().resolve().parent / \"data\"\n",
    "TRAIN_FILE = DATA_DIR / \"ratings.dat\"\n",
    "TEST_FILE = DATA_DIR / \"ratings-Test.dat\"\n",
    "GENDER_FILE = DATA_DIR / \"gender.dat\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILE)\n",
    "test_df = pd.read_csv(TEST_FILE)\n",
    "gender_df = pd.read_csv(GENDER_FILE)\n",
    "\n",
    "# Merge gender into both\n",
    "train_df = train_df.merge(gender_df, on=\"userID\", how=\"left\")\n",
    "test_df = test_df.merge(gender_df, on=\"userID\", how=\"left\")\n",
    "\n",
    "# 2) Split train/validation\n",
    "train_sub, val_sub = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a9bfe1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/1803658719.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  b_u = df.groupby(\"userID\").apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006135 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2799\n",
      "[LightGBM] [Info] Number of data points in the train set: 2576029, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.004810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yash/anaconda3/envs/da-env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 100} → Val MAE = 1.3976\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005567 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2799\n",
      "[LightGBM] [Info] Number of data points in the train set: 2576029, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.004810\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yash/anaconda3/envs/da-env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 200} → Val MAE = 1.3787\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2799\n",
      "[LightGBM] [Info] Number of data points in the train set: 2576029, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.004810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yash/anaconda3/envs/da-env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 100} → Val MAE = 1.3866\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005768 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2799\n",
      "[LightGBM] [Info] Number of data points in the train set: 2576029, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.004810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yash/anaconda3/envs/da-env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.05, 'max_depth': 10, 'n_estimators': 200} → Val MAE = 1.3762\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005897 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2799\n",
      "[LightGBM] [Info] Number of data points in the train set: 2576029, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.004810\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yash/anaconda3/envs/da-env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100} → Val MAE = 1.3760\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005885 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2799\n",
      "[LightGBM] [Info] Number of data points in the train set: 2576029, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.004810\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yash/anaconda3/envs/da-env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 200} → Val MAE = 1.3661\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.006060 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2799\n",
      "[LightGBM] [Info] Number of data points in the train set: 2576029, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.004810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yash/anaconda3/envs/da-env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 100} → Val MAE = 1.3754\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005865 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2799\n",
      "[LightGBM] [Info] Number of data points in the train set: 2576029, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.004810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yash/anaconda3/envs/da-env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200} → Val MAE = 1.3655\n",
      "Best params: {'learning_rate': 0.1, 'max_depth': 10, 'n_estimators': 200} → Val MAE = 1.3655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/1803658719.py:4: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  b_u = df.groupby(\"userID\").apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009838 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 2800\n",
      "[LightGBM] [Info] Number of data points in the train set: 3220037, number of used features: 11\n",
      "[LightGBM] [Info] Start training from score -0.003447\n",
      "✨ Final Test MAE = 2.3473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yash/anaconda3/envs/da-env/lib/python3.10/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but LGBMRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def compute_baseline(df, λ=10):\n",
    "    μ = df.rating.mean()\n",
    "    b_i = df.groupby(\"profileID\").rating.agg(lambda g: (g - μ).sum() / (len(g) + λ))\n",
    "    b_u = df.groupby(\"userID\").apply(\n",
    "        lambda g: (g.rating - μ - b_i.reindex(g.profileID).values).sum() / (len(g) + λ)\n",
    "    )\n",
    "    return μ, b_i, b_u\n",
    "\n",
    "\n",
    "def baseline_pred(u, i, μ, b_i, b_u):\n",
    "    return μ + b_u.get(u, 0.0) + b_i.get(i, 0.0)\n",
    "\n",
    "\n",
    "μ_sub, b_i_sub, b_u_sub = compute_baseline(train_sub)\n",
    "\n",
    "# ————————————————\n",
    "# 4) Residuals on train_sub\n",
    "train_sub = train_sub.copy()\n",
    "train_sub[\"baseline\"] = train_sub.apply(\n",
    "    lambda r: baseline_pred(r.userID, r.profileID, μ_sub, b_i_sub, b_u_sub), axis=1\n",
    ")\n",
    "train_sub[\"residual\"] = train_sub[\"rating\"] - train_sub[\"baseline\"]\n",
    "\n",
    "# ————————————————\n",
    "# 5) Profile & User stats (on train_sub)\n",
    "pf = train_sub.groupby(\"profileID\").agg(\n",
    "    avg_res=(\"residual\", \"mean\"),\n",
    "    count_res=(\"residual\", \"count\"),\n",
    "    avg_rating=(\"rating\", \"mean\"),\n",
    "    rating_std=(\"rating\", \"std\"),\n",
    ")\n",
    "pf[\"log_count\"] = np.log1p(pf[\"count_res\"])\n",
    "gender_counts = train_sub.pivot_table(\n",
    "    index=\"profileID\", columns=\"Gender\", values=\"residual\", aggfunc=\"count\"\n",
    ").fillna(0)\n",
    "pf[\"p_female\"] = gender_counts.get(\"F\", 0) / pf[\"count_res\"]\n",
    "pf[\"p_male\"] = gender_counts.get(\"M\", 0) / pf[\"count_res\"]\n",
    "pf[\"p_unknown\"] = gender_counts.get(\"U\", 0) / pf[\"count_res\"]\n",
    "\n",
    "uf = train_sub.groupby(\"userID\").agg(\n",
    "    u_mean=(\"rating\", \"mean\"), u_std=(\"rating\", \"std\"), u_count=(\"rating\", \"count\")\n",
    ")\n",
    "uf[\"u_logcount\"] = np.log1p(uf[\"u_count\"])\n",
    "\n",
    "# ————————————————\n",
    "# 6) Scaling\n",
    "profile_cols = [\n",
    "    \"avg_res\",\n",
    "    \"count_res\",\n",
    "    \"avg_rating\",\n",
    "    \"rating_std\",\n",
    "    \"log_count\",\n",
    "    \"p_female\",\n",
    "    \"p_male\",\n",
    "    \"p_unknown\",\n",
    "]\n",
    "user_cols = [\"u_mean\", \"u_std\", \"u_logcount\"]\n",
    "\n",
    "pf_scaler = StandardScaler().fit(pf[profile_cols].to_numpy())\n",
    "uf_scaler = StandardScaler().fit(uf[user_cols].to_numpy())\n",
    "\n",
    "\n",
    "# ————————————————\n",
    "# 7) Dataset builder\n",
    "def make_dataset(\n",
    "    df, μ, b_i, b_u, pf_df, uf_df, pf_scaler, uf_scaler, profile_cols, user_cols\n",
    "):\n",
    "    df = df.copy()\n",
    "    df[\"baseline\"] = df.apply(\n",
    "        lambda r: baseline_pred(r.userID, r.profileID, μ, b_i, b_u), axis=1\n",
    "    )\n",
    "    merged = df.merge(\n",
    "        pf_df[profile_cols], left_on=\"profileID\", right_index=True, how=\"left\"\n",
    "    ).merge(uf_df[user_cols], left_on=\"userID\", right_index=True, how=\"left\")\n",
    "    merged[profile_cols] = merged[profile_cols].fillna(0)\n",
    "    merged[user_cols] = merged[user_cols].fillna(0)\n",
    "\n",
    "    Xp = pf_scaler.transform(merged[profile_cols].to_numpy())\n",
    "    Xu = uf_scaler.transform(merged[user_cols].to_numpy())\n",
    "    X = np.hstack([Xp, Xu])\n",
    "\n",
    "    baseline = merged[\"baseline\"].to_numpy()\n",
    "    y_true = merged[\"rating\"].to_numpy()\n",
    "    y_res = y_true - baseline\n",
    "\n",
    "    return X, y_res, baseline, y_true\n",
    "\n",
    "\n",
    "X_train, y_train_res, base_train, _ = make_dataset(\n",
    "    train_sub,\n",
    "    μ_sub,\n",
    "    b_i_sub,\n",
    "    b_u_sub,\n",
    "    pf,\n",
    "    uf,\n",
    "    pf_scaler,\n",
    "    uf_scaler,\n",
    "    profile_cols,\n",
    "    user_cols,\n",
    ")\n",
    "X_val, y_val_res, base_val, y_val = make_dataset(\n",
    "    val_sub,\n",
    "    μ_sub,\n",
    "    b_i_sub,\n",
    "    b_u_sub,\n",
    "    pf,\n",
    "    uf,\n",
    "    pf_scaler,\n",
    "    uf_scaler,\n",
    "    profile_cols,\n",
    "    user_cols,\n",
    ")\n",
    "\n",
    "# ————————————————\n",
    "# 8) Grid‐search LightGBM\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"learning_rate\": [0.05, 0.1],\n",
    "    \"max_depth\": [5, 10],\n",
    "}\n",
    "best_mae, best_params = np.inf, None\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    mdl = lgb.LGBMRegressor(**params)\n",
    "    mdl.fit(X_train, y_train_res)\n",
    "    pred_res = mdl.predict(X_val)\n",
    "    preds = base_val + pred_res\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    print(f\"{params} → Val MAE = {mae:.4f}\")\n",
    "    if mae < best_mae:\n",
    "        best_mae, best_params = mae, params\n",
    "\n",
    "print(f\"Best params: {best_params} → Val MAE = {best_mae:.4f}\")\n",
    "\n",
    "# ————————————————\n",
    "# 9) Retrain on full training data\n",
    "μ_full, b_i_full, b_u_full = compute_baseline(train_df)\n",
    "\n",
    "# rebuild pf_full exactly as pf above, but using full df\n",
    "pf_full = train_df.groupby(\"profileID\").agg(\n",
    "    avg_res=(\n",
    "        \"rating\",\n",
    "        lambda g: (g - μ_full - b_i_full.reindex(g.index).values).mean(),\n",
    "    ),\n",
    "    count_res=(\"rating\", \"count\"),\n",
    "    avg_rating=(\"rating\", \"mean\"),\n",
    "    rating_std=(\"rating\", \"std\"),\n",
    ")\n",
    "pf_full[\"log_count\"] = np.log1p(pf_full[\"count_res\"])\n",
    "gender_counts_full = train_df.pivot_table(\n",
    "    index=\"profileID\", columns=\"Gender\", values=\"rating\", aggfunc=\"count\"\n",
    ").fillna(0)\n",
    "pf_full[\"p_female\"] = gender_counts_full.get(\"F\", 0) / pf_full[\"count_res\"]\n",
    "pf_full[\"p_male\"] = gender_counts_full.get(\"M\", 0) / pf_full[\"count_res\"]\n",
    "pf_full[\"p_unknown\"] = gender_counts_full.get(\"U\", 0) / pf_full[\"count_res\"]\n",
    "\n",
    "uf_full = train_df.groupby(\"userID\").agg(\n",
    "    u_mean=(\"rating\", \"mean\"), u_std=(\"rating\", \"std\"), u_count=(\"rating\", \"count\")\n",
    ")\n",
    "uf_full[\"u_logcount\"] = np.log1p(uf_full[\"u_count\"])\n",
    "\n",
    "pf_scaler_full = StandardScaler().fit(pf_full[profile_cols].to_numpy())\n",
    "uf_scaler_full = StandardScaler().fit(uf_full[user_cols].to_numpy())\n",
    "\n",
    "X_full, y_full_res, base_full, _ = make_dataset(\n",
    "    train_df,\n",
    "    μ_full,\n",
    "    b_i_full,\n",
    "    b_u_full,\n",
    "    pf_full,\n",
    "    uf_full,\n",
    "    pf_scaler_full,\n",
    "    uf_scaler_full,\n",
    "    profile_cols,\n",
    "    user_cols,\n",
    ")\n",
    "\n",
    "final_model = lgb.LGBMRegressor(**best_params)\n",
    "final_model.fit(X_full, y_full_res)\n",
    "\n",
    "# ————————————————\n",
    "# 10) Test evaluation\n",
    "X_test, y_test_res, base_test, y_test = make_dataset(\n",
    "    test_df,\n",
    "    μ_full,\n",
    "    b_i_full,\n",
    "    b_u_full,\n",
    "    pf_full,\n",
    "    uf_full,\n",
    "    pf_scaler_full,\n",
    "    uf_scaler_full,\n",
    "    profile_cols,\n",
    "    user_cols,\n",
    ")\n",
    "pred_res_test = final_model.predict(X_test)\n",
    "pred_test = base_test + pred_res_test\n",
    "test_mae = root_mean_squared_error(y_test, pred_test)\n",
    "print(f\"✨ Final Test MAE = {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "84f12975",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:51: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"rating_count\"].fillna(train_df[\"rating\"].mean(), inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:52: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"avg_rating\"].fillna(train_df[\"rating\"].mean(), inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:53: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"rating_std\"].fillna(0, inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:54: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"log_count\"].fillna(0, inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:55: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"u_count\"].fillna(0, inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:56: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"u_mean\"].fillna(train_df[\"rating\"].mean(), inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:57: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"u_std\"].fillna(0, inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:58: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"u_logcount\"].fillna(0, inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:51: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"rating_count\"].fillna(train_df[\"rating\"].mean(), inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:52: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"avg_rating\"].fillna(train_df[\"rating\"].mean(), inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:53: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"rating_std\"].fillna(0, inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:54: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"log_count\"].fillna(0, inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:55: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"u_count\"].fillna(0, inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:56: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"u_mean\"].fillna(train_df[\"rating\"].mean(), inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:57: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"u_std\"].fillna(0, inplace=True)\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/2467937834.py:58: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  df[\"u_logcount\"].fillna(0, inplace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation MAE: 1.3737\n",
      "Final Test MAE (Ridge content model): 2.2323\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, root_mean_squared_error\n",
    "\n",
    "# 1) Paths & load\n",
    "DATA_DIR = Path().resolve().parent / \"data\"\n",
    "TRAIN_FILE = DATA_DIR / \"ratings.dat\"\n",
    "TEST_FILE = DATA_DIR / \"ratings-Test.dat\"\n",
    "GENDER_FILE = DATA_DIR / \"gender.dat\"\n",
    "\n",
    "train_df = pd.read_csv(TRAIN_FILE)\n",
    "test_df = pd.read_csv(TEST_FILE)\n",
    "gender_df = pd.read_csv(GENDER_FILE)\n",
    "\n",
    "# Merge gender into both sets\n",
    "train_df = train_df.merge(gender_df, on=\"userID\", how=\"left\")\n",
    "test_df = test_df.merge(gender_df, on=\"userID\", how=\"left\")\n",
    "\n",
    "# One-hot encode user gender\n",
    "for df in (train_df, test_df):\n",
    "    df[\"is_female\"] = (df[\"Gender\"] == \"F\").astype(int)\n",
    "    df[\"is_male\"] = (df[\"Gender\"] == \"M\").astype(int)\n",
    "\n",
    "# 2) Compute profile-side stats on TRAIN ONLY\n",
    "pf = train_df.groupby(\"profileID\").agg(\n",
    "    rating_count=(\"rating\", \"count\"),\n",
    "    avg_rating=(\"rating\", \"mean\"),\n",
    "    rating_std=(\"rating\", \"std\"),\n",
    ")\n",
    "pf[\"log_count\"] = np.log1p(pf[\"rating_count\"])\n",
    "\n",
    "# 3) Compute user-side stats on TRAIN ONLY\n",
    "uf = train_df.groupby(\"userID\").agg(\n",
    "    u_count=(\"rating\", \"count\"), u_mean=(\"rating\", \"mean\"), u_std=(\"rating\", \"std\")\n",
    ")\n",
    "uf[\"u_logcount\"] = np.log1p(uf[\"u_count\"])\n",
    "\n",
    "\n",
    "# 4) Merge back to create feature‐augmented DataFrames\n",
    "def augment(df):\n",
    "    # profile features\n",
    "    df = df.merge(pf, left_on=\"profileID\", right_index=True, how=\"left\")\n",
    "    # user features\n",
    "    df = df.merge(uf, left_on=\"userID\", right_index=True, how=\"left\")\n",
    "    # fill NaNs (new profiles/users) with global means\n",
    "    df[\"rating_count\"].fillna(train_df[\"rating\"].mean(), inplace=True)\n",
    "    df[\"avg_rating\"].fillna(train_df[\"rating\"].mean(), inplace=True)\n",
    "    df[\"rating_std\"].fillna(0, inplace=True)\n",
    "    df[\"log_count\"].fillna(0, inplace=True)\n",
    "    df[\"u_count\"].fillna(0, inplace=True)\n",
    "    df[\"u_mean\"].fillna(train_df[\"rating\"].mean(), inplace=True)\n",
    "    df[\"u_std\"].fillna(0, inplace=True)\n",
    "    df[\"u_logcount\"].fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_aug = augment(train_df)\n",
    "test_aug = augment(test_df)\n",
    "\n",
    "# 5) Define feature columns\n",
    "feature_cols = [\n",
    "    \"rating_count\",\n",
    "    \"avg_rating\",\n",
    "    \"rating_std\",\n",
    "    \"log_count\",\n",
    "    \"u_count\",\n",
    "    \"u_mean\",\n",
    "    \"u_std\",\n",
    "    \"u_logcount\",\n",
    "    \"is_female\",\n",
    "    \"is_male\",\n",
    "]\n",
    "\n",
    "# 6) Split train → train_sub / val_sub\n",
    "train_sub, val_sub = train_test_split(train_aug, test_size=0.2, random_state=42)\n",
    "\n",
    "# 7) Scale features\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(train_sub[feature_cols].to_numpy())\n",
    "\n",
    "\n",
    "def prepare(df):\n",
    "    X = scaler.transform(df[feature_cols].to_numpy())\n",
    "    y = df[\"rating\"].to_numpy()\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = prepare(train_sub)\n",
    "X_val, y_val = prepare(val_sub)\n",
    "X_test, y_test = prepare(test_aug)\n",
    "\n",
    "# 8) Fit simplest regressor (Ridge)\n",
    "model = Ridge(alpha=1.0)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# 9) Evaluate on validation\n",
    "val_preds = model.predict(X_val)\n",
    "val_mae = mean_absolute_error(y_val, val_preds)\n",
    "print(f\"Validation MAE: {val_mae:.4f}\")\n",
    "\n",
    "# 10) Retrain on full training data\n",
    "X_full, y_full = prepare(train_aug)\n",
    "model.fit(X_full, y_full)\n",
    "\n",
    "# 11) Final Test evaluation\n",
    "test_preds = model.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, test_preds)\n",
    "print(f\"Final Test MAE (Ridge content model): {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cfb7e01",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "12463856",
   "metadata": {},
   "source": [
    "## KNN Based"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d60166",
   "metadata": {},
   "source": [
    "### COllaborative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c1c7cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/405023103.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  b_u = train_df.groupby(\"userID\").apply(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content‐based CF w/ sklearn KNN → Test RMSE = 2.1111\n"
     ]
    }
   ],
   "source": [
    "# 2) Compute baseline biases (μ + b_u + b_i)\n",
    "mu = train_df[\"rating\"].mean()\n",
    "# item (profile) bias\n",
    "b_i = train_df.groupby(\"profileID\")[\"rating\"].agg(\n",
    "    lambda g: (g - mu).sum() / (len(g) + 10)\n",
    ")\n",
    "# user bias\n",
    "b_u = train_df.groupby(\"userID\").apply(\n",
    "    lambda g: (g.rating - mu - b_i.reindex(g.profileID).values).sum() / (len(g) + 10)\n",
    ")\n",
    "\n",
    "\n",
    "def baseline_pred(u, i):\n",
    "    return mu + b_u.get(u, 0.0) + b_i.get(i, 0.0)\n",
    "\n",
    "\n",
    "# 3) Build the item–user residual matrix\n",
    "users = train_df[\"userID\"].unique()\n",
    "items = train_df[\"profileID\"].unique()\n",
    "user_to_idx = {u: idx for idx, u in enumerate(users)}\n",
    "item_to_idx = {i: idx for idx, i in enumerate(items)}\n",
    "\n",
    "# rows = item index, cols = user index\n",
    "rows = train_df[\"profileID\"].map(item_to_idx)\n",
    "cols = train_df[\"userID\"].map(user_to_idx)\n",
    "data = train_df.apply(lambda r: r.rating - baseline_pred(r.userID, r.profileID), axis=1)\n",
    "\n",
    "item_user_mat = csr_matrix((data, (rows, cols)), shape=(len(items), len(users)))\n",
    "\n",
    "# 4) Fit sklearn’s NearestNeighbors once\n",
    "K = 25\n",
    "knn = NearestNeighbors(\n",
    "    n_neighbors=K + 1,  # include self→we’ll skip it in prediction\n",
    "    metric=\"cosine\",\n",
    "    algorithm=\"brute\",\n",
    "    n_jobs=-1,\n",
    ")\n",
    "knn.fit(item_user_mat)\n",
    "\n",
    "\n",
    "# 5) Prediction function using sklearn’s neighbors + baseline fallback\n",
    "def predict_rating(u_id, p_id):\n",
    "    # fallback if new user/item\n",
    "    if u_id not in user_to_idx or p_id not in item_to_idx:\n",
    "        return baseline_pred(u_id, p_id)\n",
    "\n",
    "    u_idx = user_to_idx[u_id]\n",
    "    i_idx = item_to_idx[p_id]\n",
    "\n",
    "    # find neighbors\n",
    "    dist, nbrs = knn.kneighbors(item_user_mat[i_idx], return_distance=True)\n",
    "    dist = dist[0][1:]  # skip self\n",
    "    nbr_i = nbrs[0][1:]\n",
    "\n",
    "    # fetch this user’s residuals on those neighbor items\n",
    "    # .toarray() is faster than tolist for single-column\n",
    "    user_col = item_user_mat[nbr_i, u_idx].toarray().ravel()\n",
    "    mask = user_col != 0\n",
    "    if not mask.any():\n",
    "        return baseline_pred(u_id, p_id)\n",
    "\n",
    "    # weight by inverse distance\n",
    "    valid_dist = dist[mask]\n",
    "    valid_res = user_col[mask]\n",
    "    weights = 1 / (valid_dist + 1e-8)\n",
    "\n",
    "    resid_pred = (valid_res * weights).sum() / weights.sum()\n",
    "    return baseline_pred(u_id, p_id) + resid_pred\n",
    "\n",
    "\n",
    "# 6) Score on test set\n",
    "test_df[\"pred\"] = test_df.apply(lambda r: predict_rating(r.userID, r.profileID), axis=1)\n",
    "rmse = root_mean_squared_error(test_df[\"rating\"], test_df[\"pred\"])\n",
    "print(f\"Content‐based CF w/ sklearn KNN → Test RMSE = {rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ae221",
   "metadata": {},
   "source": [
    "### COntent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bdb7e4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/3380666688.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  b_u = train_df.groupby(\"userID\").apply(\n",
      "/var/folders/6b/jrqs6pc53wg_j2z0kwd4533m0000gn/T/ipykernel_74440/3380666688.py:47: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda g: dict(zip(g.profileID, g.residual)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content-based + gender features → Test RMSE = 2.1115\n"
     ]
    }
   ],
   "source": [
    "mu = train_df.rating.mean()\n",
    "b_i = train_df.groupby(\"profileID\").rating.agg(\n",
    "    lambda g, μ=mu: (g - μ).sum() / (len(g) + 10)\n",
    ")\n",
    "b_u = train_df.groupby(\"userID\").apply(\n",
    "    lambda g, μ=mu, b_i=b_i: (g.rating - μ - b_i.reindex(g.profileID).values).sum()\n",
    "    / (len(g) + 10)\n",
    ")\n",
    "\n",
    "\n",
    "def baseline(u, p):\n",
    "    return mu + b_u.get(u, 0.0) + b_i.get(p, 0.0)\n",
    "\n",
    "\n",
    "# 3) Build content features (profile side)\n",
    "pf = train_df.groupby(\"profileID\").agg(\n",
    "    avg_rating=(\"rating\", \"mean\"),\n",
    "    rating_std=(\"rating\", \"std\"),\n",
    "    rating_count=(\"rating\", \"count\"),\n",
    "    f_count=(\"Gender\", lambda x: (x == \"F\").sum()),\n",
    "    m_count=(\"Gender\", lambda x: (x == \"M\").sum()),\n",
    ")\n",
    "pf[\"p_female\"] = pf[\"f_count\"] / pf[\"rating_count\"]\n",
    "pf[\"p_male\"] = pf[\"m_count\"] / pf[\"rating_count\"]\n",
    "pf[\"log_count\"] = np.log1p(pf[\"rating_count\"])\n",
    "pf = pf.fillna(0)\n",
    "\n",
    "# which columns to use as content vector\n",
    "features = [\"avg_rating\", \"rating_std\", \"log_count\", \"p_female\", \"p_male\"]\n",
    "X_content = pf[features].to_numpy()\n",
    "\n",
    "# 4) Fit sklearn NearestNeighbors on content\n",
    "K = 25\n",
    "nn = NearestNeighbors(n_neighbors=K + 1, metric=\"cosine\", algorithm=\"brute\", n_jobs=-1)\n",
    "nn.fit(X_content)\n",
    "\n",
    "# mapping profileID <-> row index\n",
    "profile_list = pf.index.to_list()\n",
    "profile_index = {pid: i for i, pid in enumerate(profile_list)}\n",
    "\n",
    "# 5) Precompute per-user residual history\n",
    "train_df[\"residual\"] = train_df.apply(\n",
    "    lambda r: r.rating - baseline(r.userID, r.profileID), axis=1\n",
    ")\n",
    "user_hist = (\n",
    "    train_df.groupby(\"userID\")\n",
    "    .apply(lambda g: dict(zip(g.profileID, g.residual)))\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "\n",
    "# 6) Prediction function\n",
    "def predict(u, p):\n",
    "    base = baseline(u, p)\n",
    "    if p not in profile_index or u not in user_hist:\n",
    "        return base\n",
    "\n",
    "    i = profile_index[p]\n",
    "    dists, nbrs = nn.kneighbors(X_content[i].reshape(1, -1), return_distance=True)\n",
    "    dists = dists[0][1:]  # skip self\n",
    "    nbrs = nbrs[0][1:]\n",
    "    sims = 1 - dists  # cosine → similarity\n",
    "\n",
    "    hist = user_hist[u]\n",
    "    sims_list, res_list = [], []\n",
    "    for sim, j in zip(sims, nbrs):\n",
    "        pid = profile_list[j]\n",
    "        if pid in hist:\n",
    "            sims_list.append(sim)\n",
    "            res_list.append(hist[pid])\n",
    "    if not sims_list:\n",
    "        return base\n",
    "\n",
    "    sims_arr = np.array(sims_list)\n",
    "    res_arr = np.array(res_list)\n",
    "    return base + (sims_arr @ res_arr) / sims_arr.sum()\n",
    "\n",
    "\n",
    "# 7) Score on test set\n",
    "test_df[\"pred\"] = test_df.apply(lambda r: predict(r.userID, r.profileID), axis=1)\n",
    "rmse = root_mean_squared_error(test_df.rating, test_df.pred)\n",
    "print(f\"Content-based + gender features → Test RMSE = {rmse:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
