{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Content-Based Filtering for Dating Profile Recommendations\n",
    "\n",
    "This notebook implements a content-based filtering approach to recommend dating profiles to users. Content-based filtering uses characteristics of items (profiles in this case) to recommend similar items to those that a user has liked in the past.\n",
    "\n",
    "In our implementation, we combine a baseline predictor (global average + user/item biases) with a content-based approach that leverages profile features to find similarities between profiles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path().resolve().parent / \"data\"\n",
    "TRAIN_FILE = DATA_DIR / \"ratings.dat\"\n",
    "TEST_FILE = DATA_DIR / \"ratings-Test.dat\"\n",
    "GENDER_FILE = DATA_DIR / \"gender.dat\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Preprocessing\n",
    "\n",
    "We'll start by importing the necessary libraries and loading our dataset. The dataset contains:\n",
    "- Ratings given by users to different profiles\n",
    "- Gender information for users\n",
    "\n",
    "We use these to build our recommendation system. The data is split into training and validation sets to enable parameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load data\n",
    "train_df = pd.read_csv(TRAIN_FILE)\n",
    "test_df = pd.read_csv(TEST_FILE)\n",
    "gender_df = pd.read_csv(GENDER_FILE, names=[\"userID\", \"Gender\"], header=None)\n",
    "\n",
    "train_df = train_df.merge(gender_df, on=\"userID\", how=\"left\")\n",
    "\n",
    "# 2) Split into train/validation for tuning\n",
    "train_sub, val_sub = train_test_split(train_df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline Predictor\n",
    "\n",
    "Before implementing content-based filtering, we establish a baseline prediction model. This baseline consists of three components:\n",
    "\n",
    "1. **Global Mean (μ)**: The average rating across all user-profile interactions\n",
    "2. **Item Bias (b_i)**: How much better/worse a profile is rated compared to the average\n",
    "3. **User Bias (b_u)**: How much higher/lower a user tends to rate compared to the average\n",
    "\n",
    "The formula for baseline prediction is:\n",
    "\n",
    "$\\hat{r}_{ui} = \\mu + b_u + b_i$\n",
    "\n",
    "We use regularization (λ) to prevent overfitting, especially for users/profiles with few ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/b2941c6s08x9j3lg01npvbjc0000gn/T/ipykernel_7036/4057223492.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  b_i = df.groupby(\"profileID\").apply(lambda g: (g.rating - mu).sum() / (len(g) + λ))\n",
      "/var/folders/79/b2941c6s08x9j3lg01npvbjc0000gn/T/ipykernel_7036/4057223492.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  b_u = df.groupby(\"userID\").apply(user_bias)\n"
     ]
    }
   ],
   "source": [
    "def compute_baseline(df, λ=10):\n",
    "    \"\"\"Compute global mean, item biases b_i and user biases b_u on df.\"\"\"\n",
    "    mu = df[\"rating\"].mean()\n",
    "    # item bias\n",
    "    b_i = df.groupby(\"profileID\").apply(lambda g: (g.rating - mu).sum() / (len(g) + λ))\n",
    "\n",
    "    # user bias (using item bias)\n",
    "    def user_bias(g):\n",
    "        return (g.rating - mu - b_i.reindex(g.profileID).values).sum() / (len(g) + λ)\n",
    "\n",
    "    b_u = df.groupby(\"userID\").apply(user_bias)\n",
    "    return mu, b_i, b_u\n",
    "\n",
    "\n",
    "def baseline_pred(user, item, mu, b_i, b_u):\n",
    "    return mu + b_u.get(user, 0.0) + b_i.get(item, 0.0)\n",
    "\n",
    "\n",
    "# 3) Compute baseline on train_sub and residuals\n",
    "mu_sub, b_i_sub, b_u_sub = compute_baseline(train_sub, λ=10)\n",
    "train_sub = train_sub.copy()\n",
    "train_sub[\"residual\"] = train_sub.apply(\n",
    "    lambda r: r.rating - baseline_pred(r.userID, r.profileID, mu_sub, b_i_sub, b_u_sub),\n",
    "    axis=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Content-Based Model Construction\n",
    "\n",
    "The core of content-based filtering is to represent items (profiles) as feature vectors. We'll create these feature vectors from both explicit attributes (like gender distribution of raters) and implicit signals (like average rating residuals).\n",
    "\n",
    "Key aspects of our content-based model:\n",
    "\n",
    "1. We work with residuals (actual rating - baseline prediction) rather than raw ratings\n",
    "2. We create profile-level features that capture demographic and popularity aspects\n",
    "3. Features are standardized to ensure they contribute equally to similarity calculations\n",
    "4. We build a mapping structure to efficiently access profile features and user histories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Function to build content-based structures on any DataFrame with 'residual'\n",
    "def build_content_model(df):\n",
    "    # profile-level aggregates on residuals\n",
    "    pf = df.groupby(\"profileID\").agg(\n",
    "        rating_count=(\"residual\", \"count\"),\n",
    "        avg_residual=(\"residual\", \"mean\"),\n",
    "        female_count=(\"Gender\", lambda x: (x == \"F\").sum()),\n",
    "        male_count=(\"Gender\", lambda x: (x == \"M\").sum()),\n",
    "        unknown_count=(\"Gender\", lambda x: (x == \"U\").sum()),\n",
    "    )\n",
    "    total_count = pf[\"rating_count\"]\n",
    "    pf[\"unknown_count\"] = pf[\"rating_count\"] - pf[\"female_count\"] - pf[\"male_count\"]\n",
    "    pf[\"p_female\"] = pf[\"female_count\"] / total_count\n",
    "    pf[\"p_male\"] = pf[\"male_count\"] / total_count\n",
    "    pf[\"p_unknown\"] = pf[\"unknown_count\"] / total_count\n",
    "    pf[\"log_count\"] = np.log1p(pf[\"rating_count\"])\n",
    "\n",
    "    ############################\n",
    "    # Add gender ratio feature\n",
    "    # pf[\"gender_ratio\"] = pf[\"female_count\"] / (pf[\"male_count\"] + 1e-8)  # F:M ratio\n",
    "\n",
    "    # # Add additional statistics about ratings by gender\n",
    "    # gender_stats = {}\n",
    "    # for gender in [\"F\", \"M\", \"U\"]:\n",
    "    #     gender_df = df[df[\"Gender\"] == gender]\n",
    "    #     if not gender_df.empty:\n",
    "    #         gender_avg = gender_df.groupby(\"profileID\")[\"residual\"].mean()\n",
    "    #         gender_stats[f\"{gender.lower()}_avg_residual\"] = gender_avg\n",
    "\n",
    "    # # Merge these statistics with the profile dataframe\n",
    "    # for col, series in gender_stats.items():\n",
    "    #     pf = pf.join(series.rename(col), how=\"left\")\n",
    "    #     pf[col] = pf[col].fillna(0)  # Fill NAs with 0\n",
    "\n",
    "    # # Expanded feature list\n",
    "    # feats = [\n",
    "    #     \"avg_residual\",\n",
    "    #     \"log_count\",\n",
    "    #     \"p_female\",\n",
    "    #     \"p_male\",\n",
    "    #     \"p_unknown\",\n",
    "    #     \"gender_ratio\",\n",
    "    # ]\n",
    "    # feats.extend([col for col in gender_stats.keys()])\n",
    "    ###############################\n",
    "\n",
    "    # features matrix\n",
    "    feats = [\"avg_residual\", \"log_count\", \"p_female\", \"p_male\", \"p_unknown\"]\n",
    "    scaler = StandardScaler().fit(pf[feats])\n",
    "    F = scaler.transform(pf[feats])\n",
    "    norms = np.linalg.norm(F, axis=1)\n",
    "    idx_map = {pid: i for i, pid in enumerate(pf.index)}\n",
    "    # user histories: indices into F and their residuals\n",
    "    user_hist = {}\n",
    "    for uid, grp in df.groupby(\"userID\"):\n",
    "        mask = grp[\"profileID\"].isin(idx_map)\n",
    "        pids = grp.loc[mask, \"profileID\"]\n",
    "        idxs = [idx_map[pid] for pid in pids]\n",
    "        res = grp.loc[mask, \"residual\"].values\n",
    "        user_hist[uid] = (np.array(idxs), res)\n",
    "    return pf, F, norms, idx_map, user_hist, scaler\n",
    "\n",
    "\n",
    "# 5) Build content model on train_sub\n",
    "pf_sub, F_sub, norms_sub, idx_map_sub, user_hist_sub, scaler_sub = build_content_model(\n",
    "    train_sub\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Content-Based Prediction Function\n",
    "\n",
    "Now we implement the core prediction function that uses content-based similarity. The key idea is:\n",
    "\n",
    "1. For a given user-profile pair, find profiles that the user has already rated\n",
    "2. Calculate the similarity between the target profile and each rated profile using cosine similarity\n",
    "3. Predict the residual as a weighted average of residuals from the k most similar profiles\n",
    "\n",
    "This approach assumes that if a user liked a profile with certain features, they'll likely have similar reactions to profiles with similar features. We use cosine similarity in the standardized feature space to find meaningful profile relationships, regardless of scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=  5 → Validation MAE = 1.5707\n",
      "k= 10 → Validation MAE = 1.5194\n",
      "k= 20 → Validation MAE = 1.4984\n",
      "k= 50 → Validation MAE = 1.4898\n",
      "k=100 → Validation MAE = 1.5102\n",
      "Best k = 50, Validation MAE = 1.4898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/79/b2941c6s08x9j3lg01npvbjc0000gn/T/ipykernel_7036/4057223492.py:5: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  b_i = df.groupby(\"profileID\").apply(lambda g: (g.rating - mu).sum() / (len(g) + λ))\n",
      "/var/folders/79/b2941c6s08x9j3lg01npvbjc0000gn/T/ipykernel_7036/4057223492.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  b_u = df.groupby(\"userID\").apply(user_bias)\n"
     ]
    }
   ],
   "source": [
    "def predict_content(user_id, profile_id, k, F, norms, idx_map, user_hist):\n",
    "    \"\"\"Predict the residual via cosine-weighted average of k neighbors.\"\"\"\n",
    "    if profile_id not in idx_map:\n",
    "        return 0.0\n",
    "    tgt = idx_map[profile_id]\n",
    "    v_t = F[tgt]\n",
    "    n_t = norms[tgt]\n",
    "    hist = user_hist.get(user_id)\n",
    "    if hist is None:\n",
    "        return 0.0\n",
    "    idxs, res = hist\n",
    "    sims = (F[idxs] @ v_t) / (norms[idxs] * n_t + 1e-8)\n",
    "    top = np.argsort(-sims)[:k]\n",
    "    sims_k, res_k = sims[top], res[top]\n",
    "    if sims_k.sum() <= 0:\n",
    "        return res_k.mean()\n",
    "    return np.dot(sims_k, res_k) / sims_k.sum()\n",
    "\n",
    "\n",
    "# 6) Tune k on validation set\n",
    "best_mae, best_k = np.inf, None\n",
    "for k in [5, 10, 20, 50, 100]:\n",
    "    # predict on val_sub: baseline + content\n",
    "    preds = []\n",
    "    for _, r in val_sub.iterrows():\n",
    "        base = baseline_pred(r.userID, r.profileID, mu_sub, b_i_sub, b_u_sub)\n",
    "        res = predict_content(\n",
    "            r.userID, r.profileID, k, F_sub, norms_sub, idx_map_sub, user_hist_sub\n",
    "        )\n",
    "        preds.append(base + res)\n",
    "    mae = mean_absolute_error(val_sub[\"rating\"], preds)\n",
    "    print(f\"k={k:3d} → Validation MAE = {mae:.4f}\")\n",
    "    if mae < best_mae:\n",
    "        best_mae, best_k = mae, k\n",
    "\n",
    "print(f\"Best k = {best_k}, Validation MAE = {best_mae:.4f}\")\n",
    "\n",
    "# 7) Retrain on full training set\n",
    "mu_full, b_i_full, b_u_full = compute_baseline(train_df, λ=10)\n",
    "train_df_full = train_df.copy()\n",
    "train_df_full[\"residual\"] = train_df_full.apply(\n",
    "    lambda r: r.rating\n",
    "    - baseline_pred(r.userID, r.profileID, mu_full, b_i_full, b_u_full),\n",
    "    axis=1,\n",
    ")\n",
    "pf_full, F_full, norms_full, idx_map_full, user_hist_full, scaler_full = (\n",
    "    build_content_model(train_df_full)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parameter Tuning and Model Evaluation\n",
    "\n",
    "The parameter k (number of neighbors) significantly affects the performance of our model:\n",
    "\n",
    "1. Small k: Predictions are influenced by only a few highly similar profiles (more specific but potentially unstable)\n",
    "2. Large k: Predictions are influenced by many profiles (more stable but potentially less relevant)\n",
    "\n",
    "We'll tune k by evaluating the model on our validation set using Mean Absolute Error (MAE). After finding the optimal k value, we'll retrain the model on the full training set and evaluate on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MAE at k=50: 1.5936\n"
     ]
    }
   ],
   "source": [
    "# 8) Evaluate on true test set\n",
    "preds_test = []\n",
    "for _, r in test_df.iterrows():\n",
    "    # best_k = 50\n",
    "    base = baseline_pred(r.userID, r.profileID, mu_full, b_i_full, b_u_full)\n",
    "    res = predict_content(\n",
    "        r.userID, r.profileID, best_k, F_full, norms_full, idx_map_full, user_hist_full\n",
    "    )\n",
    "    preds_test.append(base + res)\n",
    "\n",
    "test_mae = mean_absolute_error(test_df[\"rating\"], preds_test)\n",
    "print(f\"Test MAE at k={best_k}: {test_mae:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "da-rec",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
